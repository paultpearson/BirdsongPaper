{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a CNN to Identify Birdsong Snippets, February 28, 2019\n",
    "\n",
    "This notebook contains code for designing and training a convolutional neural network to identify snippets chosen from birdsong samples. Then, snippets and WAV files are identified using the trained model from a training set, a validation set, and a test set of WAV files. This notebook follows the processes described in the book chapter \"Using Neural Networks to Identify Bird Species from Birdsong Samples\" by Russell Houpt, Mark Pearson, Paul Pearson, Taylor Rink, Sarah Seckler, Darin Stephenson, and Allison VanderStoep. This work was done at Hope College between 2016 and 2018.\n",
    "\n",
    "First, we load some useful packages and options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress = True, precision = 8)\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "df = pd.read_csv('fileListdf.csv')\n",
    "speciesList = [\"Bananaquit\",\"Roadside Hawk\",\"Green Violetear\",\"Buff-breasted Wren\"]\n",
    "df_train = pd.read_csv('traindf.csv')\n",
    "df_valid = pd.read_csv('validdf.csv')\n",
    "df_test = pd.read_csv('testdf.csv')\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Convolution1D, MaxPooling1D, Dropout, Flatten\n",
    "from keras.callbacks import History\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras import regularizers\n",
    "\n",
    "saveDirectory = '/home/stephenson/Documents/birdsongs/Publication Notebooks/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function <b>get_onehot</b> will take a snippet file id and return the one-hot encoded correct species for that snippet.\n",
    "\n",
    "The <b>generator</b> is used to select batches of snippets for training and validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_onehot(fileid,slist=speciesList,df=df):\n",
    "    idn = int(fileid.split('-')[0])\n",
    "    return np.identity((len(slist)))[slist.index(df[df.id == idn]['name'].values[0])].astype(int)\n",
    "\n",
    "def generator(filenames,datafile, batch_size,slist=speciesList):\n",
    "    batch_features = np.zeros((batch_size, 64, 1323, 1))\n",
    "    batch_labels = np.zeros((batch_size,len(slist)))\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            index= random.choice(range(filenames.shape[0]))\n",
    "            trainfile = filenames[index]\n",
    "            batch_features[i] = np.array(datafile[trainfile]).reshape(64,1323,1)\n",
    "            batch_labels[i] = get_onehot(trainfile)\n",
    "        yield (batch_features, batch_labels)         \n",
    "\n",
    "        \n",
    "h5_file = h5py.File(saveDirectory+\"snippets.hdf5\",\"r\")\n",
    "filesTrain = np.array([name for name in h5_file if h5_file[name].attrs['speciesName'] in speciesList \n",
    "                      and int(name.split('-')[0]) in df_train['id'].values])\n",
    "filesValid = np.array([name for name in h5_file if h5_file[name].attrs['speciesName'] in speciesList \n",
    "                      and int(name.split('-')[0]) in df_valid['id'].values])\n",
    "filesTest = np.array([name for name in h5_file if h5_file[name].attrs['speciesName'] in speciesList \n",
    "                      and int(name.split('-')[0]) in df_test['id'].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we set up a convolutional neural network using Keras as a front end for TensorFlow. This simple CNN is a Keras sequential model, consisting of, in order,\n",
    "<UL>\n",
    "    <LI> a $9\\times 9$ convolutional layer with 16 channels, \n",
    "    <LI> a $4\\times 4$ Max Pooling layer, \n",
    "    <LI> a $3\\times 3$ convolutional layer with 16 channels, \n",
    "    <LI> a $4\\times 4$ Max Pooling layer, \n",
    "    <LI> a dense layer 100 nodes,\n",
    "    <LI> a second dense layer with 100 notes, \n",
    "    <LI> an output layer with 4 nodes. \n",
    "        </UL>\n",
    "Each of the convolutional or dense layers is followed by a ReLU non-linearity, except for the output layer, which is followed by a softmax function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()\n",
    "total = []\n",
    "epochs = 3\n",
    "train_batch=100\n",
    "train_steps=1000\n",
    "val_batch=100\n",
    "val_steps=200\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = 9, \n",
    "                 strides = 1, padding='same',\n",
    "                 activation = 'relu',use_bias=True,\n",
    "                 kernel_initializer='random_uniform',\n",
    "                 input_shape =(64,1323,1)))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=4, strides=None, \n",
    "                       padding='valid', data_format=None))\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = 3, \n",
    "                 strides = 1, padding='same',\n",
    "                 activation = 'relu',use_bias=True,\n",
    "                 kernel_initializer='random_uniform'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=4, strides=None, \n",
    "                       padding='valid', data_format=None))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(len(speciesList), activation = 'softmax'))\n",
    "model.compile(optimizer = 'adadelta',\n",
    "              loss ='categorical_crossentropy',\n",
    "              metrics =['accuracy'])\n",
    "keras.optimizers.Adadelta()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is for training the model and recording the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=model.fit_generator(generator(filenames=filesTrain,\n",
    "                                   datafile=h5_file,batch_size=train_batch), \n",
    "                         steps_per_epoch=train_steps, epochs=epochs,\n",
    "                         callbacks=[history],\n",
    "                         validation_data=generator(filenames=filesValid, \n",
    "                                                   datafile=h5_file,\n",
    "                                                   batch_size=val_batch),\n",
    "                         validation_steps = val_steps)\n",
    "total.append(hist.history)\n",
    "\n",
    "train_accuracy     =     total[0]['acc']\n",
    "train_accuracy     =     np.asarray(train_accuracy)\n",
    "train_loss         =     total[0]['loss']\n",
    "train_loss         =     np.asarray(train_loss)\n",
    "\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will use the trained model to predict the identity of each snippet in the training, validation, and testing set, and will also use snippet voting to predict the identity of each WAV file in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model(\"model.h5\")\n",
    "\n",
    "# mode = \"Training Set\"\n",
    "# mode = \"Validation Set\"\n",
    "mode = \"Test Set\"\n",
    "\n",
    "if mode == \"Training Set\":\n",
    "    info = df_train[['id','filename','name']].values\n",
    "    files = filesTrain\n",
    "elif mode == \"Validation Set\":\n",
    "    info = df_valid[['id','filename','name']].values\n",
    "    files = filesValid\n",
    "elif mode == \"Test Set\":\n",
    "    info = df_test[['id','filename','name']].values\n",
    "    files = filesTest\n",
    "\n",
    "\n",
    "d = {}\n",
    "snippet_conf = np.zeros((len(speciesList),len(speciesList))).astype(int)\n",
    "species_conf = np.zeros((len(speciesList),len(speciesList))).astype(int)\n",
    "for sample in info:\n",
    "    idn=sample[0]\n",
    "    d[idn] = np.zeros(len(speciesList)).astype(int)\n",
    "    files_idn = [x for x in files if int(x.split('-')[0])==idn]\n",
    "    cor = get_onehot(files_idn[0])\n",
    "    cornum = np.argmax(cor)\n",
    "    for file in files_idn:\n",
    "        f = model.predict(np.array(h5_file[file]).reshape(1,64,1323,1))\n",
    "        d[idn][np.argmax(f)]+=1\n",
    "    snippet_conf[cornum,:] += d[idn]\n",
    "    species_conf[cornum,np.argmax(d[idn])] += 1\n",
    "correct = np.sum(np.diag(species_conf))\n",
    "total =  np.sum(species_conf)\n",
    "snip_correct = np.sum(np.diag(snippet_conf))\n",
    "snip_total = np.sum(snippet_conf)\n",
    "print(mode+\" Accuracy:\")\n",
    "print(\"Snippets: \"+str(snip_correct)+\" correct out of \"+str(snip_total)\n",
    "      +\". \"+str(np.round(100*snip_correct/snip_total,1))+\" percent correct.\")\n",
    "print(\"WAV files: \"+str(correct)+\" correct out of \"+str(total)+\". \"\n",
    "      +str(np.round(100*correct/total,1))+\" percent correct.\")\n",
    "print(\"Snippet Confusion Matrix:\")\n",
    "print(snippet_conf)\n",
    "print(\"WAV File Confusion Matrix:\")\n",
    "print(species_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
